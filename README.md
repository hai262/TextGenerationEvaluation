# üìù Text Generation Evaluation

## Project Overview
Text Generation Evaluation is a comprehensive project designed to assess the performance of text generation models using advanced evaluation metrics. This project enables users to compare generated text outputs against reference texts and compute metrics such as BLEU, ROUGE, and others to provide a robust analysis of model performance in natural language generation tasks.

## Key Features
- **Advanced Evaluation Metrics:** Implements metrics including BLEU, ROUGE, and others for quantitative evaluation of text quality.
- **Data Preprocessing:** Automates the cleaning and preprocessing of both generated and reference texts for accurate comparison.
- **Interactive Visualizations:** Uses Plotly and Matplotlib to create interactive charts that showcase evaluation scores and performance trends.
- **Modular Design:** Easily integrates with various text generation models and allows the addition of new evaluation metrics.
- **User-Friendly Interface:** Built with Streamlit for real-time testing and experimentation with text generation outputs.

## Technologies & Tools
- **Programming Language:** Python
- **Data Processing:** Pandas, NumPy, NLTK, SpaCy
- **Evaluation Metrics:** BLEU, ROUGE, METEOR (optional)
- **Visualization:** Matplotlib, Plotly
- **Web Application:** Streamlit
- **Version Control:** Git, GitHub

## Installation
Follow these steps to set up and run the Text Generation Evaluation project locally:

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/hai262/TextGenerationEvaluation.git
   cd TextGenerationEvaluation
